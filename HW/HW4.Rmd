---
date: "5/15/2019"
output: html_document
header-includes:
   - \usepackage[table,xcdraw]{xcolor}
urlcolor: blue
---

```{r setup, include=FALSE}
options(scipen = 999)
knitr::opts_chunk$set(echo = TRUE)
```

# CS7290 Causal Modeling in Machine Learning: Homework 4

## Submission guidelines

## Background

## 1. Probability of Neccessity and Sufficiency

Consider the following data comparing purchases on an e-commerce website with high and low exposure to promotions. 

```{r, echo=FALSE}
library(tidyverse)
n11 <- 30
n01 <- 69130
n10 <- 16
n00 <- 59010
tot <- n11 + n01 + n10 + n00
y_marg = (n11 + n10) / tot
x_marg = (n11 + n01) / tot
py1_x0 = n10/ (n10 + n00)
py1_x1 = n11/ (n11 + n01)
px1_y0 = n01/ (n00 + n01)
px1_y1 = n11/ (n11 + n10)

model <- function(pq, py){
    nx = rbinom(1, 1, x_marg)
    nq = rbinom(1, 1, pq)
    ny = rbinom(1, 1, py)
    x = nx
    q = nq
    y = ((x * q) + ny > 0)
    return(y)
}

func <- function(pq, px){
    n <- 100000# 1000000
    return(sum(Vectorize(model)(rep(pq, n), rep(px, n)))/n)
    #sum(map_dbl(1:n, ~ model(pq, px))) / n 
}
# Find good values for ny and nx
func(.00023, .00023)
y_marg

px= x_marg
pq= .00023
py = .00023
```

|          |                      | Promotional  Exposure |                        |
|----------|----------------------|-----------------------|------------------------|
|          |                      | High (X=1)            | Low (X=0)              |
| Purchase | Yes (Y = 1)          | `r n11`               | `r n10`                |
|          | No (Y = 0)           | `r n01`               | `r n00`                |


|          |                      | Promotional  Exposure |                        |                                |
|----------|----------------------|-----------------------|------------------------|------------------------------  |
|          |                      | High (X=1)            | Low (X=0)              |                                |
| Purchase | Yes (Y = 1)          | `r round(n11/tot, 4)` | `r round(n10/tot, 4)`  | P(Y=1) = `r round(y_marg, 4)`  |
|          | No (Y = 0)           | `r round(n01/tot, 4)` | `r round(n00/tot, 4)`  | P(Y=0) = `r 1 - round(y_marg,4)`|
|          |                      | P(Y=1) =`r round(x_marg, 4)`  |P(Y=0) =`r 1 - round(x_marg, 4)`|                 |

|                   | Conditional probabilities |
|-------------------|---------------------------|
| P(Y = 1 \| X = 0)  | `r py1_x0`  |
| P(Y = 1 \| X = 1)  | `r py1_x1 ` |
| P(X = 1 \| Y = 0)  | `r px1_y0` |
| P(X = 1 \| Y = 1)  | `r px1_y1` |

Given these data, we wish to estimate the probabilities that high exposure to promotions was a necessary (or sufficient, or both) cause of purchase.  We assume monotonicity -- exposure to promotions didn't cause anyone NOT to make a purchase.

We assume the following DAG.  Here, the vari

```{r xqy, echo=FALSE, message=FALSE, warning=FALSE, fig.width=1.5, fig.height=2}
library(bnlearn)
dag <- model2network('[X][Q][Y|X:Q]')
graphviz.plot(dag)
```

We assume that purchases $Y$ has the following simple disjunctive mechanism:


\begin{align} 
\mathbf{M} =
\left\{\begin{matrix}
n_x &\sim &\text{BernoulliBool}(p=0.5433931)\\ 
n_q &\sim &\text{BernoulliBool}(p=0.05)\\ 
n_y &\sim &\text{BernoulliBool}(p=.0077)\\ 
x &= &n_x \\ 
q &= &n_q \\
y &= &(x \wedge q) \vee n_y
\\ 
\end{matrix}\right. \nonumber
\end{align}


### A. Calculate the probability of necessity: $P(Y_{0} = 0| X = 1, Y = 1)$

##### Solution:

First infer the distribution of $n_y$ and $n_q$ give X = 1 and Y = 1.  
$P(Y_{0} = 0| X = 1, Y = 1)$

```{r echo=FALSE}

joints = rep(NA, 8)
joints[1] = (1-px) * (1-pq) * (1-py)
joints[2] = (1-px) * (1-pq) * py
joints[3] = (1-px) * pq * (1-py)
joints[4] = (1-px) * pq * py
joints[5] = px * (1-pq) * (1-py)
joints[6] = px * (1-pq) * py
joints[7] = px * pq * (1-py)
joints[8] = px * pq * py
```

Make a joint probability table:

| $n_x$ | $n_q$ | $n_y$ | x | q | y | prob                                                 |
--------|-------|-------|---|---|---|------------------------------------------------------|
|  0    |   0   |  0    | 0 | 0 | 0 | `r round(joints[1], 4)` |
|  0    |   0   |  1    | 0 | 0 | 1 | `r round(joints[2], 4)`     |
|  0    |   1   |  0    | 0 | 1 | 0 | `r round(joints[3], 4)`     |
|  0    |   1   |  1    | 0 | 1 | 1 | `r round(joints[4], 4)`         |
|  1    |   0   |  0    | 1 | 0 | 0 | `r round(joints[5], 4)`     |
|  1    |   0   |  1    | 1 | 0 | 1 | `r round(joints[6], 4)`         |
|  1    |   1   |  0    | 1 | 1 | 1 | `r round(joints[7], 4)`         |
|  1    |   1   |  1    | 1 | 1 | 1 | `r round(joints[8], 4)`             |

So if Y is 1 and X is 1, then our outcomes of interest are 

```{r}
posterior <- rep(NA, 3)
posterior[1] = joints[6]/sum(joints[6:8])
posterior[2] = joints[7]/sum(joints[6:8])
posterior[3] = joints[8]/sum(joints[6:8])
```

| $n_x$ | $n_q$ | $n_y$ | x | q | y | conditional prob                         |
--------|-------|-------|---|---|---|------------------------------------------|
|  1    |   0   |  1    | 1 | 0 | 1 | `r round(posterior[1], 4)` |
|  1    |   1   |  0    | 1 | 1 | 1 | `r round(posterior[2], 4)` |
|  1    |   1   |  1    | 1 | 1 | 1 | `r round(posterior[3], 4)` |

So do we pass these noise terms back into the intervention.  Note, we can ignore $n_x$ in this model since $x$ is intervened on.

| $n_q$ | $n_y$ | do(x = 0) | q | y | conditional prob                         |
|-------|-------|-----------|---|---|------------------------------------------|
|   0   |  1    | 0         | 0 | 1 | `r round(posterior[1], 4)` |
|   1   |  0    | 0         | 1 | 0 | `r round(posterior[2], 4)` |
|   1   |  1    | 0         | 1 | 1 | `r round(posterior[3], 4)` |

There is only one outcome where y is 0.  So the PN is `r posterior[2]`.



### B. Get probability of sufficiency $P(Y_{1} = 1| X = 0, Y = 0)$



#### Solution 

There is only one joint outcome where X == 0 and Y == 0.

| $n_x$ | $n_q$ | $n_y$ | x | q | y | prob                    |
--------|-------|-------|---|---|---|-------------------------|
|  0    |   0   |  0    | 0 | 0 | 0 | `r round(joints[1]/(joints[1]+joints[3]), 4)` |
|  0    |   1   |  0    | 0 | 1 | 0 | `r round(joints[3]/(joints[1]+joints[3]), 4)` |

#### $n_q == 0, n_y == 0$: 
In the counterfactual X is set to 1.  If $n_q = q = 0$ and therefore $int(x \wedge q) = 0$. If $n_y=0$ is 0 then $int((x \wedge q) \vee n_y) =0$. 

#### $n_q == 1, n_y == 0$:
In the counterfactual X is set to 1.  If $n_q = q = 1$ and therefore $int(x \wedge q) = 1$. So $int((x \wedge q) \vee n_y) =1$

Therefore, PS = 0 \* `r joints[1] /(joints[1]+joints[3])` + 1 \* `r joints[3]/(joints[1]+joints[3])` =`r joints[3]/(joints[1]+joints[3])`

## 2. Probability of Neccessity and Sufficiency, and Identifiability

Typically we don't know the whole structural model.  We would only have the statistical table above,  Assume only the structural assignment for Y is known.  

If X is exogenous and Y is monotonic relative to X, then the probabilities PN, PS, and PNS are all identifiable and are given by 

\begin{align*}
\text{PNS} &= P(Y=1|X =1) - P(Y=1|X =0) \\
\text{PN} &= \frac{PNS}{P(Y=1|X=1)} \\
\text{PS} &= \frac{PNS}{P(Y=0|X=0)}
\end{align*}

### Find probability of neccessity and sufficiency in problem 1

#### Solution

`r py1_x1 - py1_x0`

### Find PN and PS using just PNS and the conditional probabilities

#### Solution

* PN = `r (py1_x1 - py1_x0) / py1_x1`
* PS = `r (py1_x1 - py1_x0) / (1-py1_x0)`

## 3. Mediation

Suppose you are a developer for a freemium subscription content platform.  Your company did an A/B test for a new feature, designed to increase conversions to a paid premium subscription.  The variables here are $X \in \{0, 1\}$ for whether or not a user was exposed to the feature, and $Y \in \{0, 1\}$ for conversion.

Based on some analysis and domain knowledge, you come up with the following model.

$$
\mathbb{C} = \left\{\begin{matrix}
X =& N_X\\ 
T =& 3*X + N_T\\ 
E =& 2*T + 8*X + N_E \\ 
Y =& I(E > 10 + N_C) 
\end{matrix}\right.
$$

Here $T$ is "thrash".  Since the new feature changes the website's UX, "thrash" quantifies the time and effort the user has to spend familiarizing themselves with the new UX.  $E$ is engagement.  The model assumes that the more the user engages with the site the more likely they are to convert. I is an indicator function, it returns 1 if engagement (E) > 10, 0 otherwise. Though the A/B test ties to estimate the causal effect of X on Y, T and E are mediators of that effect. You want to know how much the feature drives conversions directly through engagement, and how much is just due to thrash (which might have negative consequences on other outcomes not explicitly included in this model).

$N_X$ comes from a fair-coin flip.  All of the other noise variables are normal distributions with mean 0.  However, for simplicity, we are going to assume noise variables all have a variance/standard deviation of 0.  In other words, for our purposes you can assign a value of 0 to all the noise terms.

1. Calculate the total effect of the feature on conversions.

Solution: $2 * 3 +  1 *8 > 10 == 1$

2. Calculate the natural indirect effect (NIE).  NIE is the expected change in conversions, given no exposure to the feature, but set thrash (T) at the level it would take if one was exposed to the feature.

Solution:  Thrash would be 2.  $2 * 3 is 6 < 10$.  So NIE = 0.

3. Calculate the controlled direct effect (CDE) when thrash is 0.  A CDE is the effect you get when holding a mediator at a fixed value.

Solution: $1 * 8 < 10 == 0$

4. Compute the natural indirect effect.  Reverse NIE is expected *change* in conversions, given thrash (T) being fixed at feature exposure levels, but then setting X to 0.  (HINT: "change" means that going from 0 to 1 means the effect is a positive number, going from 1 to 0 means the effect has a negative number.)

Solution: $8 * 0 + 2 * 3 < 10$.  Reverse NIE is -1

5. Compute the natural direct effect (ND) using the following formula: Total effect = NDE - reverse NIE.  Explain what the implications of this is to the analysis of this feature?

Solution: NDE = 0.  This means that people exposed to the feature would not have converted if it were not for the additional thrash caused by the feature.

6.  Discussion:  If the noise variables were not degenerate (meaning the didn't have non-zero variance), how would this have affected the calculations and the conclusion about the NDE?

7.  Suppose instead we used the following model.

$$
\mathbb{C} = \left\{\begin{matrix}
X =& N_X\\
\vec{U} =& N_U \\
T =& 2*X + N_T\\ 
E =& 3*S + 7*W + N_E \\
Y =& I(g(E, U, N_C) > \epsilon) 
\end{matrix}\right.
$$

Here, $\vec{U}$ is a vector of user-related features.  $g$ is a deep neural network that takes as input engagement (E) as well as these other user features and the noise term, and outputs a value. $\epsilon$ is a threshold.  Describe in clear terms how this would change the above analysis, if at all.

## Effect of the treatment on the treated

Suppose you work for a car-sharing service like Uber.  You find that many drivers are making decisions in ways that are sub-optimal for the drivers, often missing low-hanging fruit (e.g. picking up riders closer to where they live, or choosing to drive in areas that have less demand and yet more traffic than others).  If the drivers made better decisions about when and where to drive, they could make more money with a similar amount of effort.

The company hires a statistical consulting company that samples some drivers for a training study.  The goal of the study is to test whether a driver training program will lead drivers to make better decisions.  Drivers in the study are randomly assigned to $X = 1$ (recieved optimal driving training) or $X = 0$ (recieved basic training that doesn't encourage optimal descision-making).  The outcome variable $Y$ is the amount of revenue the drivers earn in the study period.

Let $Y_{X=1}$ be the revenue earned under exposure to the optimal training and $Y_{X=0}$ be revenue earned under exposure to baseline training.  The study showed that the training is highly effected ($E(Y_{X=1} - Y_{X=0}) > \epsilon$) where $\epsilon$ is some stastical significance threshold.

Your team is debating whether or not you should build that training program into the mobile app.  Drivers would opt-in to recieve training and guidance while driving.  It would be quite expensive in terms of time and engineering resources to create this app.  However, you colleagues say that the expected revenue $E(Y_{X=1} - Y_{X=0})$ would more than make up for the cost.

You argue that most drivers who would opt-in are already highly motivated drivers.  You think they would go on to drive more optimally by learning from their own experience, research, seeking out successful drivers, etc.

To demonstrate this, you will estimate the effect of the treatment on the treated (ETT) is $E(Y_{X=1} - Y_{X=0}|X=1)$. In plain English this is the expected difference in earned revenue from those who recieved training relative to what revenue would have been had they not recieved training.

The terms $Y_{X=1}$ and $Y_{X=0}$ in $E(Y_{X=1} - Y_{X=0}|X=1)$ are causal variables, in order to estimate them, you need to convert them into variables than can be estimated directly from data.  

The following mathematical derivations show you how to calculate ETT given $Z$, a set of valid adjustment variables that satisfy the backdoor criterion w.r.t $X$ and $Y$.

Firstly, the following is true according to the basic rules of conditional probability.

\begin{align}
P(Y_{x}=y|X = x') &= \sum_{z}P(Y_{x}=y, Z = z| X = x')\\
&=\sum_{z}P(Y_{x}=y|Z = z, X = x')P(Z = z| X = x')
\end{align}

Secondly, the counterfactual implication of the backdoor criterion is $X \perp Y_{x} |Z$.  This means that $P(Y_{x}=y|Z = z, X = x') = P(Y_{x}=y|Z = z, X = x)$, because conditional on $Z$, the probability of $Y_{x}$ doesn't respond to $X$.  This leads to the next simplification:

\begin{align}
P(Y_{x}=y|X = x') &= \sum_{z}P(Y_{x}=y, Z = z| X = x')\\
&=\sum_{z}P(Y_{x}=y|Z = z, X = x')P(Z = z| X = x') \\
& =\sum_{z}P(Y_{x}=y|Z = z, X = x)P(Z = z| X = x') \\
&=\sum_{z}P(Y = y|Z = z, X = x)P(Z = z| X = x')
\end{align}

The last step is because covariate adjustment means adjusting for Z allows conditioning on X = x can stand in for do(X = x).

All of the terms in the last line are estimable from data. All you need to do is calculate expectations.

(Sicheng, please come up with a fake dataset that will work here.)

